# Markov Decision Process

Reinforcement learning (RL) is a general framework where agents learn to perform actions in an environment so as to maximize a reward. 
The Markov Decision Process (MDP) is a mathematical framework for modeling reinforcement learning problems.

A MDP has the following components:

- **State and observation**: A state $s$ is a complete description of the state of the world. There is no information about the world which is hidden from the state. An observation $o$ is a partial description of a state, which may omit information. The set of states is called the state space, denoted as $\mathcal{S}$. It could be discrete (e.g., chess board) or continuous (e.g., position and velocity of a robot).
- **Action**: An action $a$ is an observable choice made by the agent. The set of actions is called the action space. Action space could be discrete (e.g., move left, right, up, down) or continuous vectors (e.g., move along a certain speed vector). The set of actions is called the action space, denoted as $\mathcal{A}$.
- **Transition probability**: The transition probability $P(s'|s,a)$ is the probability of transitioning to state $s'$ given that the agent took action $a$ in state $s$. When state space is continuous, the transition probability is a probability density function.
- **Reward function**: The reward function $R(s,a)$ is the reward received by the agent for taking action $a$ in state $s$.

![MDP](./rl.assets/mdp.png)


We observe the tuple $(s_0, a_0, s_1, a_1, \cdots, s_t, a_t)$ from MDP as the agent interacts with the environment.

## Policy

A policy is a rule used by an agent to decide what actions to take. 

A deterministic policy $\pi: \mathcal{S} \rightarrow \mathcal{A}$ is a mapping from states to actions. It represents the agent's behavior.

A stochastic policy $\pi(a|s)$ is a conditional probability distribution over actions given states. That means, given a state $s$, the policy $\pi$ will take action $a$ with probability $\pi(a|s)$. When the action space is continuous, the policy is a probability density function.

In deep RL, we deal with parameterized policies: 

$$
a_t \sim \pi_{\theta}(a|s_t) \text{ or } a_t = \pi_{\theta}(s_t)
$$

where $\theta$ is usually a neural network. In this course, we focus on the continuous state and action space with stochastic policy. In the following of this chapter, unless otherwise specified, we will presume the state and action spaces are continuous and $\pi(\cdot|s)$ is a density over action space.

Given a stochastic policy $\pi_{\theta}(a|s)$, the MDP trajectory $\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \cdots, s_t, a_t, r_t)$ is generated by

$$
a_t \sim \pi_{\theta}(a|s_t) \\
s_{t+1} \sim P(s_{t+1}|s_t, a_t)\\
r_t = R(s_t, a_t)
$$

## Value Function

The goal of reinforcement learning is to learn a policy $\pi_{\theta}$ that maximizes the expected cumulative reward

$$
J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}} \left[R(\tau)\right], \text{ where } R(\tau) = \sum_{t=0}^{\infty} \gamma^t r_t
$$

and $\gamma$ is the discount factor. 

There are four main functions of note here.

- **On-Policy Value Function**, $V^{\pi}(s)$, which gives the expected return if you start in state $s$ and always act according to policy $\pi$:

$$
V^{\pi}(s) = \mathbb{E}_{\tau \sim \pi}[R(\tau)| s_0 = s]
$$

- **On-Policy Action-Value Function**, $Q^{\pi}(s,a)$, which gives the expected return if you start in state $s$, take an arbitrary action $a$ (which may not have come from the policy), and then forever after act according to policy $\pi$:

$$
Q^{\pi}(s,a) = \mathbb{E}_{\tau \sim \pi}[R(\tau)| s_0 = s, a_0 = a]
$$

- **Optimal Value Function**, $V^*(s)$, which gives the expected return if you start in state $s$ and always act according to the optimal policy in the environment:

$$
V^*(s) = \max_{\pi} \mathbb{E}_{\tau \sim \pi}[R(\tau)| s_0 = s]
$$

- **Optimal Action-Value Function**, $Q^*(s,a)$, which gives the expected return if you start in state $s$, take an arbitrary action $a$, and then forever after act according to the optimal policy in the environment:

$$
Q^*(s,a) = \max_{\pi} \mathbb{E}_{\tau \sim \pi}[R(\tau)| s_0 = s, a_0 = a]
$$

## Bellman Equation

All four of the value functions obey special self-consistency equations called **Bellman equations**.

The Bellman equations for the on-policy value functions are

$$
\begin{align*}
V^{\pi}(s) &= \mathbb{E}_{a \sim \pi, s'\sim P}[r(s,a) + \gamma V^{\pi}(s')], \\
Q^{\pi}(s,a) &= \mathbb{E}_{s'\sim P, a'\sim \pi}[r(s,a) + \gamma Q^{\pi}(s',a')],
\end{align*}
$$

The Bellman equations for the optimal value functions are

$$
\begin{align*}
V^*(s) &= \max_{a} \mathbb{E}_{s'\sim P}[r(s,a) + \gamma V^*(s')], \\
Q^*(s,a) &= \mathbb{E}_{s'\sim P}[r(s,a) + \gamma \max_{a'} Q^*(s',a')],
\end{align*}
$$

The above equations are essentially [dynamic programming](../chapter_algorithms/dynamic_programming.md) which we covered in the earlier chapter.


## Advantage Functions

Sometimes in RL, we donâ€™t need to describe how good an action is in an absolute sense, but only how much better it is than others on average. That is to say, we want to know the relative advantage of that action. We make this concept precise with the advantage function.

$$
A^{\pi}(s,a) = Q^{\pi}(s,a) - V^{\pi}(s)
$$

The advantage function $A^{\pi}(s,a)$ corresponding to a policy $\pi$ describes how much better it is to take a specific action $a$ in state $s$, over randomly selecting an action according to $\pi(\cdot|s)$, assuming you act according to $\pi$ forever after.  




